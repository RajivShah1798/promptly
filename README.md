# Promptly
## Introduction
Promptly is a simple online service that helps you quickly find answers in your documents whether those are PDFs, text files, or other kinds of files. Imagine having a virtual assistant who has read all your paperwork and can instantly tell you whatâ€™s inside each document.
1) **Upload Your Files:** You can upload documents (like PDFs or text files) through Promptly.
2) **Ask Questions:** Whenever you want information, just type in your question.
3) **Get Accurate Answers:** Promptly will search through the documents youâ€™ve uploaded and give you a direct answer, almost like an expert who has read everything.

---

## Current Problems:
ğŸš« Manual Document Analysis â€“ Professionals spend hours searching and summarizing documents.  
ğŸš« Keyword Search Limitations â€“ Basic search tools fail to provide context-aware answers.  
ğŸš« Scalability Issues â€“ Many solutions struggle with large document volumes.  

---

## Proposed Solutions:
âœ… Persistent Multi-Document Memory â€“ Users can create chatbot instances that retain knowledge.  
âœ… Cross-Document Referencing â€“ AI synthesizes insights across multiple files.  
âœ… Role-Based Customization â€“ Chatbot adapts to different roles (Legal, Finance, IT, HR).  
âœ… On-Premise & Cloud Deployment â€“ Enterprise-friendly with security and compliance in mind 

---

## Business Value Proposition
ğŸ¢ For Teams & Enterprises â€“ A centralized AI knowledge assistant for fast and reliable document retrieval.  
ğŸ“– For Individuals â€“ A personal AI assistant for organizing and querying private notes and study materials.  

--- 
## Project Directory Structure

```
â”œâ”€â”€ assets/                     # Diagrams and visual assets
â”‚   â”œâ”€â”€ process_user_queries_dag.png  # User Query Pipeline Workflow Diagram
â”‚   â”œâ”€â”€ rag_data_pipeline_dag.png     # Data Pipeline Workflow Diagram
â”‚   â”œâ”€â”€ Data Pipeline Architecture.png
â”‚   â”œâ”€â”€ Model Training & Deployment Architecture.png
â”‚   â”œâ”€â”€ Drift Detection.png
â”‚   â”œâ”€â”€ pipeline_optimization.png
â”‚
â”œâ”€â”€ data_pipeline/              # Data processing and ingestion pipeline
â”‚   â”œâ”€â”€ dags/                   # Airflow DAGs for pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ dataPipeline.py     # User Queries DAG
â”‚   â”‚   â”œâ”€â”€ rag_data_pipeline.py # Document Processing DAG
â”‚   â”‚   â”œâ”€â”€ scripts/            # Scripts for data preprocessing and ingestion
â”‚   â”‚   â”‚   â”œâ”€â”€ email_utils.py  # Email notifications
â”‚   â”‚   â”‚   â”œâ”€â”€ upload_data_GCS.py  # GCS Uploading
â”‚   â”‚   â”‚   â”œâ”€â”€ data_preprocessing/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ check_pii_data.py  # PII Detection
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ validate_schema.py  # Schema Validation
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ data_utils.py  # Query Cleaning Functions
â”‚   â”‚   â”‚   â”œâ”€â”€ supadb/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ supabase_utils.py  # Supabase Integration
â”‚   â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ validate_schema.py  # Schema Validation
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rag_utils.py  # Chunking & Embeddings
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_data_pii_redact.py  # Unit tests for PII detection and redaction
â”‚   â”‚   â”‚   â”œâ”€â”€ test_rag_pipeline.py  # Unit tests for the RAG document chunking pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ test_user_queries.py  # Unit tests for the user queries processing pipeline
â”‚   â”œâ”€â”€ config.py               # API Keys & Configurations
â”‚   â”œâ”€â”€ README.md               # Data Pipeline Documentation
â”‚
â”œâ”€â”€ model/                      # Model serving, testing, and deployment
â”‚   â”œâ”€â”€ serve.py                # FastAPI server for serving the model
â”‚   â”œâ”€â”€ deploy_server.py        # Deployment script for Google Cloud Vertex AI
â”‚   â”œâ”€â”€ Dockerfile              # Docker configuration for containerizing the model server
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies for the model
â”‚   â”œâ”€â”€ tests/                  # Unit tests for the model and APIs
â”‚   â”‚   â”œâ”€â”€ api_test.py         # Test script for the REST API
â”‚   â”‚   â””â”€â”€ sdk_test.py         # Test script for SDK integration
â”‚   â”œâ”€â”€ README.md               # Model Directory Documentation
â”‚
â”œâ”€â”€ model_pipeline/             # Model training and fine-tuning pipeline
â”‚   â”œâ”€â”€ training/               # Training scripts and notebooks
â”‚   â”‚   â”œâ”€â”€ promptly-finetuning.ipynb  # Model training Jupyter notebook
â”‚   â”‚   â”œâ”€â”€ README.md           # Training-specific documentation
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ bias_detection.py   # Script for detecting bias while training the model
â”‚   â”‚   â”œâ”€â”€ load_data.py        # Load data from Supabase
â”‚   â”‚   â”œâ”€â”€ streamlit_ui.py     # Streamlit user interface for the app
â”‚   â”œâ”€â”€ mlflow/
â”‚   â”‚   â”œâ”€â”€ Dockerfile          # Docker file for setting up MLflow in GCP Instance
â”‚   â”œâ”€â”€ README.md               # Model Pipeline Documentation
â”‚
â”œâ”€â”€ data/                       # Data storage and processing
â”‚   â”œâ”€â”€ rag_documents/          # Original PDFs & Text Files
â”‚   â”œâ”€â”€ preprocessed_docs_chunks.csv  # Cleaned & Chunked Data
â”‚   â”œâ”€â”€ preprocessed_user_data.csv    # Processed User Queries
â”‚
â”œâ”€â”€ .github/workflows/          # CI/CD workflows
â”‚   â”œâ”€â”€ README.md               # GitHub Actions Documentation
â”‚
â”œâ”€â”€ .dvc/                       # DVC Configuration
â”œâ”€â”€ .gitignore                  # Git ignore file
â”œâ”€â”€ .dvcignore                  # DVC ignore file
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # Global project documentation
```
---

## Data Source
### 1. User Queries
- Source: Retrieved from the conversations table in Supabase.
- Description: This table contains user-generated queries, which we have pre-filled with custom data to simulate various interaction scenarios.

### 2. Documents
- Source: Focused on IT specifications, we have curated data from publicly available requirements documents.
- Description: We have selectively gathered documents that provide detailed IT specifications, particularly from the PURE dataset, which comprises 79 publicly available natural language requirements documents collected from the web.
- Reference: [https://zenodo.org/records/5195084](https://zenodo.org/records/5195084)

---

## System Architecture

### 1. Data Pipeline:
![Data Pipeline Architecture](/assets/Data%20Pipeline%20Architecture.png)
- The data pipeline automates document ingestion, preprocessing, and storage in Supabase.
- For detailed documentation, refer to the [Data Pipeline README](./data_pipeline/README.md).

### 2. Model Training and Deployment Pipeline
![Model Training & Deployment Architecture](/assets/Model%20Training%20&%20Deployment%20Architecture.png)
- The model training pipeline handles fine-tuning and evaluation of the model. For more details, refer to the [Model Training README](./model_pipeline/README.md).
- The model serving is the deployment of model to Vertex AI. For more details, refer to the [Model Serving README](./model/README.md).

### 3. Data Drift Detection
![Data Drift Detection Architecture](/assets/Drift%20Detection.png)
- User dominance is verified to check if the drift is caused by a single user or skewed usage pattern.
- If drift is detected, it triggers downstream actions:
- Drift trend analysis to monitor changes over time.
- Model retraining is triggered automatically if needed.
- The entire process is automated via an Apache Airflow DAG, which runs once a day.
- Upon completion or detection, an email notification is sent to alert stakeholders.

---

## Key Features

1. **Cross-Document Retrieval:** Enables querying across multiple documents for context-aware answers.
2. **Fine-Tuned Model:** Uses a fine-tuned version of `Qwen/Qwen2.5-0.5B-Instruct` for text generation tasks.
3. **Cloud Deployment:** Supports deployment to Google Cloud Vertex AI for scalable inference.
4. **CI/CD Integration:** Automates model training and deployment using GitHub Actions.

---


### Project proposal [View Here](https://docs.google.com/document/d/1ZARzFI9JG95JxkLO9lD2LJZfr6xIisEh2SVzuhyPN3M/edit?usp=sharing)

## **Contact**

For any questions or issues, reach out to the Promptly team:

- **Ronak Vadhaiya** - vadhaiya.r@northeastern.edu
- **Sagar Bilwal** - bilwal.sagar@northeastern.edu
- **Rajiv Shah** - shah.rajiv1702@gmail.com
- **Kushal Shankar** - kushalshankar03@gmail.com