# Promptly
## Introduction
Promptly is a simple online service that helps you quickly find answers in your documents whether those are PDFs, text files, or other kinds of files. Imagine having a virtual assistant who has read all your paperwork and can instantly tell you whatâ€™s inside each document.
1) **Upload Your Files:** You can upload documents (like PDFs or text files) through Promptly.
2) **Ask Questions:** Whenever you want information, just type in your question.
3) **Get Accurate Answers:** Promptly will search through the documents youâ€™ve uploaded and give you a direct answer, almost like an expert who has read everything.

---

## Current Problems:
ğŸš« Manual Document Analysis â€“ Professionals spend hours searching and summarizing documents.  
ğŸš« Keyword Search Limitations â€“ Basic search tools fail to provide context-aware answers.  
ğŸš« Scalability Issues â€“ Many solutions struggle with large document volumes.  

---

## Proposed Solutions:
âœ… Persistent Multi-Document Memory â€“ Users can create chatbot instances that retain knowledge.  
âœ… Cross-Document Referencing â€“ AI synthesizes insights across multiple files.  
âœ… Role-Based Customization â€“ Chatbot adapts to different roles (Legal, Finance, IT, HR).  
âœ… On-Premise & Cloud Deployment â€“ Enterprise-friendly with security and compliance in mind 

---

## Business Value Proposition
ğŸ¢ For Teams & Enterprises â€“ A centralized AI knowledge assistant for fast and reliable document retrieval.  
ğŸ“– For Individuals â€“ A personal AI assistant for organizing and querying private notes and study materials.  

--- 
## Project Directory Structure

```
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ process_user_queries_dag.png  # User Query Pipeline Worflow Diagram
â”‚   â”œâ”€â”€ rag_data_pipeline_dag.png  # Data Pipeline Workflow Diagram
â”‚
â”œâ”€â”€ data_pipeline/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ dataPipeline.py  # User Queries DAG
â”‚   â”‚   â”œâ”€â”€ rag_data_pipeline.py  # Document Processing DAG
â”‚   â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”‚   â”œâ”€â”€ email_utils.py  # Email notifications
â”‚   â”‚   â”‚   â”œâ”€â”€ upload_data_GCS.py  # GCS Uploading
â”‚   â”‚   â”‚   â”œâ”€â”€ data_preprocessing/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ check_pii_data.py  # PII Detection
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ validate_schema.py  # Schema Validation
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ data_utils.py  # Query Cleaning Functions
â”‚   â”‚   â”‚   â”œâ”€â”€ supadb/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ supabase_utils.py  # Supabase Integration
â”‚   â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ validate_schema.py  # Schema Validation
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rag_utils.py  # Chunking & Embeddings
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_data_pii_redact.py  # Unit tests for PII detection and redaction
â”‚   â”‚   â”‚   â”œâ”€â”€ test_rag_pipeline.py  # Unit tests for the RAG document chunking pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ test_user_queries.py  # Unit tests for the user queries processing pipeline
â”‚   â”œâ”€â”€ config.py  # API Keys & Configurations
â”‚   â”œâ”€â”€ README.md  # Data Pipeline Documentation
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ rag_documents/  # Original PDFs & Text Files
â”‚   â”œâ”€â”€ preprocessed_docs_chunks.csv/  # Cleaned & Chunked Data
â”‚   â”œâ”€â”€ preprocessed_user_data.csv  # Processed User Queries
â”‚
â”œâ”€â”€ .dvc/  # DVC Configuration
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dvcignore
â”œâ”€â”€ README.md  # Project Overview
â”œâ”€â”€ requirements.txt  # Dependencies
```
---

## Data Source
### 1. User Queries
- Source: Retrieved from the conversations table in Supabase.
- Description: This table contains user-generated queries, which we have pre-filled with custom data to simulate various interaction scenarios.

### 2. Documents
- Source: Focused on IT specifications, we have curated data from publicly available requirements documents.
- Description: We have selectively gathered documents that provide detailed IT specifications, particularly from the PURE dataset, which comprises 79 publicly available natural language requirements documents collected from the web.
- Reference: [https://zenodo.org/records/5195084](https://zenodo.org/records/5195084)

---

## System Architecture

### 1. Data Pipeline:
- Readme continues [here](./data_pipeline/README.md)

### 2. Model Training and Deployment Pipeline
- Work in Progress

### 3. Data Drift Detection
- Work in Progress


### Project proposal [View Here](https://docs.google.com/document/d/1ZARzFI9JG95JxkLO9lD2LJZfr6xIisEh2SVzuhyPN3M/edit?usp=sharing)

## **Contact**

For any questions or issues, reach out to the Promptly team:

- **Ronak Vadhaiya** - vadhaiya.r@northeastern.edu
- **Sagar Bilwal** - bilwal.sagar@northeastern.edu
- **Kushal Shankar** - kushalshankar03@gmail.com
- **Rajiv Shah** - shah.rajiv1702@gmail.com
